{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e05cf3e0"
      },
      "source": [
        "# Task\n",
        "Realizar o ajuste fino de um modelo DeepLabV3 para detecção de culturas e ervas daninhas usando o dataset disponível em \"https://www.kaggle.com/datasets/ravirajsinh45/crop-and-weed-detection-data-with-bounding-boxes\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c456a7cf"
      },
      "source": [
        "## Download e extração dos dados\n",
        "Baixar o dataset do Kaggle e extraí-lo para um diretório local.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "kaggle_cred_path = '/content/kaggle.json'\n",
        "\n",
        "if os.path.exists(kaggle_cred_path):\n",
        "    # Define a variável de ambiente para que o Kaggle encontre o arquivo\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = os.path.dirname(kaggle_cred_path)\n",
        "\n",
        "    # Define as permissões corretas (somente o proprietário pode ler/escrever)\n",
        "    !chmod 600 {kaggle_cred_path}\n",
        "\n",
        "    print(f\"Credenciais do Kaggle carregadas com sucesso de {kaggle_cred_path}\")\n",
        "else:\n",
        "    print(f\"Erro: Arquivo 'kaggle.json' não encontrado em {kaggle_cred_path}\")\n",
        "    print(\"Por favor, faça o upload do seu 'kaggle.json' para o diretório /content/ (raiz).\")"
      ],
      "metadata": {
        "id": "D5F3EuuhTeHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc09f347"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Create a directory for the dataset\n",
        "dataset_dir = 'crop_weed_dataset'\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "# Download the dataset using Kaggle API\n",
        "dataset_name = 'ravirajsinh45/crop-and-weed-detection-data-with-bounding-boxes'\n",
        "download_command = f'kaggle datasets download -d {dataset_name} -p {dataset_dir}'\n",
        "print(f\"Executing download command: {download_command}\")\n",
        "os.system(download_command)\n",
        "\n",
        "# Unzip the dataset\n",
        "zip_file_path = os.path.join(dataset_dir, os.path.basename(dataset_name).replace('/', '_') + '.zip')\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_dir)\n",
        "\n",
        "print(f\"Dataset downloaded and extracted to {dataset_dir}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b1c45a4"
      },
      "source": [
        "## Preparação dos dados\n",
        "Carregar as imagens e as anotações (bounding boxes) e prepará-las para o treinamento do modelo DeepLabV3. Isso pode incluir redimensionamento, normalização e criação de máscaras de segmentação a partir das bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "data_dir = 'crop_weed_dataset/agri_data/data'\n",
        "mask_dir = 'data/masks'\n",
        "\n",
        "# IMPORTANTE: Para nossas máscaras, queremos 0:fundo, 1:crop, 2:weed.\n",
        "class_map = {\n",
        "    0: 1,  # crop\n",
        "    1: 2   # weed\n",
        "}\n",
        "# --------------------------------\n",
        "os.makedirs(mask_dir, exist_ok=True)\n",
        "\n",
        "print(\"Iniciando criação de máscaras a partir de anotações .txt (YOLO)...\")\n",
        "\n",
        "# Iterar sobre todos os arquivos de anotação .txt\n",
        "for txt_file in os.listdir(data_dir):\n",
        "    if not txt_file.endswith('.txt'):\n",
        "        continue\n",
        "\n",
        "    # Encontrar a imagem correspondente\n",
        "    img_filename = txt_file.replace('.txt', '.jpeg')\n",
        "    img_path = os.path.join(data_dir, img_filename)\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"Aviso: Imagem não encontrada para a anotação '{txt_file}'\")\n",
        "        continue\n",
        "\n",
        "    # Obter as dimensões da imagem\n",
        "    try:\n",
        "        # Usar OpenCV para ler as dimensões\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            raise Exception(f\"Não foi possível ler a imagem {img_path}\")\n",
        "        height, width, _ = image.shape\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao ler imagem {img_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Criar uma máscara em branco (0s) com as dimensões da imagem\n",
        "    mask = np.zeros((height, width), dtype=np.uint8)\n",
        "\n",
        "    # Abrir o arquivo .txt de anotação\n",
        "    with open(os.path.join(data_dir, txt_file), 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            try:\n",
        "                parts = line.strip().split()\n",
        "                class_id = int(parts[0])\n",
        "                x_center_norm = float(parts[1])\n",
        "                y_center_norm = float(parts[2])\n",
        "                width_norm = float(parts[3])\n",
        "                height_norm = float(parts[4])\n",
        "\n",
        "                # Obter o valor do pixel para a máscara (1 para crop, 2 para weed)\n",
        "                pixel_value = class_map.get(class_id)\n",
        "                if pixel_value is None:\n",
        "                    continue # Ignora classes que não estão no nosso map\n",
        "\n",
        "                # Desnormalizar as coordenadas\n",
        "                box_width = width_norm * width\n",
        "                box_height = height_norm * height\n",
        "                x_center = x_center_norm * width\n",
        "                y_center = y_center_norm * height\n",
        "\n",
        "                # Converter para coordenadas (xmin, ymin, xmax, ymax)\n",
        "                xmin = int(x_center - (box_width / 2))\n",
        "                ymin = int(y_center - (box_height / 2))\n",
        "                xmax = int(x_center + (box_width / 2))\n",
        "                ymax = int(y_center + (box_height / 2))\n",
        "\n",
        "                # Garantir que as coordenadas estejam dentro dos limites da imagem\n",
        "                xmin = max(0, xmin)\n",
        "                ymin = max(0, ymin)\n",
        "                xmax = min(width, xmax)\n",
        "                ymax = min(height, ymax)\n",
        "\n",
        "                # Desenhar o retângulo preenchido na máscara\n",
        "                # cv2.rectangle(imagem, (xmin, ymin), (xmax, ymax), cor, preenchimento)\n",
        "                cv2.rectangle(mask, (xmin, ymin), (xmax, ymax),\n",
        "                              color=int(pixel_value), thickness=cv2.FILLED)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao processar linha '{line}' no arquivo {txt_file}: {e}\")\n",
        "\n",
        "    # Salvar a máscara como um arquivo .png\n",
        "    mask_pil = Image.fromarray(mask)\n",
        "    mask_filename = img_filename.replace('.jpeg', '.png')\n",
        "    mask_pil.save(os.path.join(mask_dir, mask_filename))\n",
        "\n",
        "print(f\"\\nProcessamento concluído. Máscaras salvas em {mask_dir}\")"
      ],
      "metadata": {
        "id": "37ZCtExtU0Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Dividir os dados em conjuntos de treinamento e validação.\n",
        "\n",
        "* Listar Arquivos: Criar uma lista de todos os nomes de arquivo base (ex: ['agri_0_1002', 'agri_0_1007', ...]).\n",
        "\n",
        "* Dividir a Lista: Usar sklearn.model_selection.train_test_split para dividir essa lista de nomes."
      ],
      "metadata": {
        "id": "9jfF3hh8X34r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Imagens originais estão aqui:\n",
        "image_dir = data_dir\n",
        "\n",
        "# dataset final organizado:\n",
        "output_dataset_dir = 'dataset_preparado'\n",
        "# ---------------------------------\n",
        "\n",
        "# Criar a estrutura de pastas de destino\n",
        "train_img_path = os.path.join(output_dataset_dir, 'images', 'train')\n",
        "val_img_path = os.path.join(output_dataset_dir, 'images', 'val')\n",
        "train_mask_path = os.path.join(output_dataset_dir, 'masks', 'train')\n",
        "val_mask_path = os.path.join(output_dataset_dir, 'masks', 'val')\n",
        "\n",
        "os.makedirs(train_img_path, exist_ok=True)\n",
        "os.makedirs(val_img_path, exist_ok=True)\n",
        "os.makedirs(train_mask_path, exist_ok=True)\n",
        "os.makedirs(val_mask_path, exist_ok=True)\n",
        "\n",
        "\n",
        "# 1. Listar arquivos (vamos usar as imagens .jpeg como referência)\n",
        "try:\n",
        "    image_files_jpeg = [f for f in os.listdir(image_dir) if f.endswith('.jpeg')]\n",
        "    # Obter os nomes base (sem extensão)\n",
        "    file_bases = [f.replace('.jpeg', '') for f in image_files_jpeg]\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERRO: Não encontrei o diretório de imagens em '{image_dir}'\")\n",
        "    raise\n",
        "\n",
        "if not file_bases:\n",
        "    print(f\"ERRO: Nenhuma imagem .jpeg encontrada em '{image_dir}'\")\n",
        "    raise\n",
        "\n",
        "# 2. Dividir a lista de nomes\n",
        "train_files, val_files = train_test_split(file_bases, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Total de pares (imagem/máscara): {len(file_bases)}\")\n",
        "print(f\"Conjunto de treino: {len(train_files)}\")\n",
        "print(f\"Conjunto de validação: {len(val_files)}\")\n",
        "\n",
        "# 3. Função auxiliar para COPIAR os arquivos\n",
        "def copy_files(file_list, source_img_dir, source_mask_dir, dest_img_dir, dest_mask_dir):\n",
        "    count = 0\n",
        "    missing_masks = 0\n",
        "    for file_base in file_list:\n",
        "        img_src = os.path.join(source_img_dir, file_base + '.jpeg')\n",
        "        mask_src = os.path.join(source_mask_dir, file_base + '.png') # Assumindo máscara .png\n",
        "\n",
        "        # Verificar se ambos os arquivos existem antes de copiar\n",
        "        if os.path.exists(img_src) and os.path.exists(mask_src):\n",
        "            shutil.copy(img_src, os.path.join(dest_img_dir, file_base + '.jpeg'))\n",
        "            shutil.copy(mask_src, os.path.join(dest_mask_dir, file_base + '.png'))\n",
        "            count += 1\n",
        "        elif not os.path.exists(mask_src):\n",
        "            print(f\"Aviso: Máscara não encontrada para '{file_base}.png' em {source_mask_dir}\")\n",
        "            missing_masks += 1\n",
        "        else:\n",
        "            print(f\"Aviso: Imagem não encontrada para '{file_base}.jpeg' em {source_img_dir}\")\n",
        "    return count, missing_masks\n",
        "\n",
        "# 4. Copiar os arquivos para a nova estrutura\n",
        "print(\"Copiando arquivos de treino...\")\n",
        "num_train, missing_train = copy_files(train_files, image_dir, mask_dir, train_img_path, train_mask_path)\n",
        "\n",
        "print(\"Copiando arquivos de validação...\")\n",
        "num_val, missing_val = copy_files(val_files, image_dir, mask_dir, val_img_path, val_mask_path)\n",
        "\n",
        "print(\"\\n--- Concluído! ---\")\n",
        "print(f\"{num_train} pares de treino copiados para '{output_dataset_dir}/'\")\n",
        "print(f\"{num_val} pares de validação copiados para '{output_dataset_dir}/'\")\n",
        "if (missing_train + missing_val) > 0:\n",
        "    print(f\"ATENÇÃO: {missing_train + missing_val} máscaras não foram encontradas e seus pares foram ignorados.\")\n",
        "\n",
        "print(\"\\nSua nova estrutura de dataset está pronta em 'dataset_preparado/'.\")"
      ],
      "metadata": {
        "id": "p4I0eejdXU1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Augmentation\n",
        "Para segmentação, é crucial que transformações espaciais (como rotação ou flip) sejam aplicadas exatamente da mesma forma na imagem e na sua máscara. A biblioteca albumentations faz isso automaticamente.\n",
        "\n",
        "* Para Treino: Apliquei várias aumentações para tornar o modelo robusto.\n",
        "\n",
        "* Para Validação: Apliquei apenas o pré-processamento básico\n",
        "(redimensionamento e normalização) para obter uma avaliação consistente."
      ],
      "metadata": {
        "id": "SJvBOwBaYFUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Definir um tamanho padrão para o qual todas as imagens serão redimensionadas\n",
        "IMG_SIZE = 256\n",
        "\n",
        "# Definir as médias e desvios padrão do ImageNet (usados para pré-treinamento)\n",
        "MEAN = [0.485, 0.456, 0.406]\n",
        "STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "# 1. Pipeline de transformações para o conjunto de TREINO\n",
        "train_transform = A.Compose([\n",
        "    # Redimensiona mantendo o aspecto e preenchendo (Pad) ou cortando (RandomCrop)\n",
        "    A.LongestMaxSize(max_size=IMG_SIZE),\n",
        "    A.PadIfNeeded(min_height=IMG_SIZE, min_width=IMG_SIZE, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
        "\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "\n",
        "    # Aumentações de cor (aplicadas SOMENTE à imagem)\n",
        "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.3),\n",
        "\n",
        "    # Normalização e conversão para Tensor\n",
        "    A.Normalize(mean=MEAN, std=STD),\n",
        "    ToTensorV2(), # Converte imagem e máscara para Tensores PyTorch\n",
        "])\n",
        "\n",
        "# 2. Pipeline de transformações para o conjunto de VALIDAÇÃO\n",
        "val_transform = A.Compose([\n",
        "    # Apenas redimensiona para o tamanho esperado pelo modelo\n",
        "    A.LongestMaxSize(max_size=IMG_SIZE),\n",
        "    A.PadIfNeeded(min_height=IMG_SIZE, min_width=IMG_SIZE, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
        "\n",
        "    # Normalização e conversão para Tensor\n",
        "    A.Normalize(mean=MEAN, std=STD),\n",
        "    ToTensorV2(),\n",
        "])"
      ],
      "metadata": {
        "id": "rauJkTu4XkAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Criação do Dataset e DataLoaders (PyTorch)\n",
        "Agora, criarei uma classe Dataset personalizada que o PyTorch pode usar. Ela irá:\n",
        "\n",
        "1. Encontrar os pares de imagem/máscara.\n",
        "\n",
        "2. Carregá-los do disco.\n",
        "\n",
        "3. Aplicar as transformações que definimos acima."
      ],
      "metadata": {
        "id": "0c1OySetYd0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CropWeedDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Lista apenas os arquivos de imagem\n",
        "        self.images = [f for f in os.listdir(image_dir) if f.endswith('.jpeg')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Nome do arquivo de imagem\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        # Nome do arquivo de máscara correspondente\n",
        "        mask_name = img_name.replace('.jpeg', '.png')\n",
        "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "        # Carregar imagem (OpenCV lê em BGR, convertemos para RGB)\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Carregar máscara (em tons de cinza, exatamente como foi salva)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Nossas classes na máscara são 0 (fundo), 1 (crop), 2 (weed)\n",
        "\n",
        "        # Aplicar transformações\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "            # A função de perda (CrossEntropyLoss) espera que a máscara\n",
        "            # seja do tipo Long (inteiro de 64 bits), não Float.\n",
        "            mask = mask.long()\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# --- Caminhos para os dados preparados ---\n",
        "# fiz outras variaveis pra ter certeza\n",
        "TRAIN_IMG_DIR = 'dataset_preparado/images/train'\n",
        "TRAIN_MASK_DIR = 'dataset_preparado/masks/train'\n",
        "VAL_IMG_DIR = 'dataset_preparado/images/val'\n",
        "VAL_MASK_DIR = 'dataset_preparado/masks/val'\n",
        "\n",
        "# 1. Instanciar os Datasets\n",
        "train_dataset = CropWeedDataset(\n",
        "    image_dir=TRAIN_IMG_DIR,\n",
        "    mask_dir=TRAIN_MASK_DIR,\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "val_dataset = CropWeedDataset(\n",
        "    image_dir=VAL_IMG_DIR,\n",
        "    mask_dir=VAL_MASK_DIR,\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "# 2. Instanciar os DataLoaders\n",
        "BATCH_SIZE = 8 # Ajuste conforme a memória da sua GPU\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True, # Embaralhar dados de treino é crucial\n",
        "    num_workers=4, # Use múltiplos processos para carregar dados\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False, # Não há necessidade de embaralhar a validação\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"DataLoaders prontos!\")\n",
        "print(f\"Lotes de Treino ({len(train_dataset)} imagens): {len(train_loader)} lotes de tamanho {BATCH_SIZE}\")\n",
        "print(f\"Lotes de Validação ({len(val_dataset)} imagens): {len(val_loader)} lotes de tamanho {BATCH_SIZE}\")"
      ],
      "metadata": {
        "id": "k9C36YeLYbEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Carregar o Modelo DeepLabv3\n",
        "Vamos usar um modelo DeepLabv3 pré-treinado (com backbone ResNet-50) do torchvision e adaptá-lo para o nosso problema. O modelo original foi treinado no dataset COCO com 21 classes. Precisamos trocar a \"cabeça\" (camada de classificação final) para que ela produza saídas para as nossas 3 classes:\n",
        "\n",
        "* 0: Background\n",
        "\n",
        "* 1: Crop\n",
        "\n",
        "* 2: Weed"
      ],
      "metadata": {
        "id": "CDDJY1klY4Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
        "\n",
        "# --- Configurações Iniciais ---\n",
        "NUM_CLASSES = 3  # (0: Fundo, 1: Crop, 2: Weed)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 2  # Comece com 10-20 e ajuste conforme necessário\n",
        "\n",
        "print(f\"Usando dispositivo: {DEVICE}\")\n",
        "\n",
        "# 1. Carregar o modelo DeepLabv3 pré-treinado\n",
        "# Usamos os pesos mais recentes (Weights.COCO_WITH_VOC_LABELS_V1)\n",
        "weights = DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\n",
        "model = deeplabv3_resnet50(weights=weights)\n",
        "\n",
        "\"\"\"\n",
        "2. Modificar a camada de classificação final (classifier)\n",
        "-> O DeepLabv3 no torchvision usa um 'classifier' com 5 camadas\n",
        "-> A última camada (índice 4) é um Conv2d que projeta para 21 classes (COCO)\n",
        "-> Vamos substituí-la por uma nova Conv2d para nossas 3 classes.\n",
        "\"\"\"\n",
        "\n",
        "# Obter o número de canais de entrada da camada antiga\n",
        "in_channels = model.classifier[4].in_channels\n",
        "\n",
        "# Criar a nova camada de classificação\n",
        "model.classifier[4] = nn.Conv2d(\n",
        "    in_channels,\n",
        "    NUM_CLASSES,\n",
        "    kernel_size=1, # Kernel 1x1 é suficiente para a classificação final\n",
        "    stride=1\n",
        ")\n",
        "\n",
        "# 3. Mover o modelo para o dispositivo (GPU ou CPU)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "print(\"Modelo DeepLabv3 carregado e modificado para 3 classes.\")"
      ],
      "metadata": {
        "id": "ou5WXk1DY4BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Definir Função de Perda, Otimizador e Loop de Treinamento\n",
        "Agora, definimos como o modelo aprenderá.\n",
        "\n",
        "* Função de Perda (Loss): nn.CrossEntropyLoss. É a escolha padrão para segmentação multiclasse. Ela compara os logits (saída bruta do modelo) com as máscaras de inteiros (0, 1, 2).\n",
        "\n",
        "* Otimizador: torch.optim.Adam. Um otimizador robusto e popular."
      ],
      "metadata": {
        "id": "bZqaBybhZBvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir Perda e Otimizador\n",
        "# CrossEntropyLoss é ideal, pois nossas máscaras são 0, 1, 2\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "'''\n",
        " (Opcional) Um scheduler para ajustar a taxa de aprendizado\n",
        " scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n",
        "'''\n",
        "\n",
        "print(\"Função de perda (CrossEntropy) e Otimizador (Adam) definidos.\")"
      ],
      "metadata": {
        "id": "g8L2q0OKY_Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##O Loop de Treinamento e Validação\n",
        "Este é o coração do processo. Vamos criar uma função para uma época de treino (train_fn) e uma para validação (val_fn).\n",
        "\n",
        "Nota Importante sobre a Saída do DeepLab: O modelo deeplabv3 do torchvision retorna um dicionário. A saída de segmentação principal está na chave 'out'."
      ],
      "metadata": {
        "id": "4NW01_n2ZHrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_fn(loader, model, optimizer, loss_fn):\n",
        "    \"\"\"Executa uma época de treinamento.\"\"\"\n",
        "    loop = tqdm(loader, desc=\"Treinando\")\n",
        "\n",
        "    total_loss = 0.0\n",
        "    model.train() # Coloca o modelo em modo de treinamento\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.to(device=DEVICE) # targets já são (N, H, W) e tipo Long\n",
        "\n",
        "        # 1. Forward pass\n",
        "        # A saída do deeplab é um dicionário, pegamos a chave 'out'\n",
        "        predictions = model(data)['out'] # Saída é (N, C=3, H, W)\n",
        "\n",
        "        # 2. Calcular a perda\n",
        "        loss = loss_fn(predictions, targets)\n",
        "\n",
        "        # 3. Backward pass e otimização\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Atualiza a barra de progresso\n",
        "        total_loss += loss.item()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    print(f\"-> Fim da Época de Treino. Perda Média: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "def val_fn(loader, model, loss_fn):\n",
        "    \"\"\"Executa uma época de validação.\"\"\"\n",
        "    loop = tqdm(loader, desc=\"Validando\", leave=False) # 'leave=False' para limpar após terminar\n",
        "\n",
        "    model.eval() # Coloca o modelo em modo de avaliação (desliga dropout, etc.)\n",
        "    total_val_loss = 0.0\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "\n",
        "    with torch.no_grad(): # Desliga o cálculo de gradientes\n",
        "        for batch_idx, (data, targets) in enumerate(loop):\n",
        "            data = data.to(device=DEVICE)\n",
        "            targets = targets.to(device=DEVICE)\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = model(data)['out']\n",
        "\n",
        "            # Calcular perda\n",
        "            loss = loss_fn(predictions, targets)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            # Calcular Acurácia de Pixel\n",
        "            # Pega o índice da classe com maior logit (canal 0, 1 ou 2)\n",
        "            preds_labels = predictions.argmax(dim=1) # (N, H, W)\n",
        "            num_correct += (preds_labels == targets).sum()\n",
        "            num_pixels += torch.numel(targets) # Total de pixels (N * H * W)\n",
        "\n",
        "    avg_loss = total_val_loss / len(loader)\n",
        "    pixel_accuracy = (num_correct / num_pixels) * 100\n",
        "\n",
        "    print(f\"-> Fim da Validação. Perda Média: {avg_loss:.4f}, Acurácia de Pixel: {pixel_accuracy:.2f}%\")\n",
        "    return avg_loss, pixel_accuracy\n",
        "\n",
        "\n",
        "# --- O Loop Principal de Treinamento ---\n",
        "\n",
        "print(\"\\n--- Iniciando o Treinamento ---\")\n",
        "\n",
        "# Variáveis para salvar o melhor modelo\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nÉpoca [{epoch+1}/{NUM_EPOCHS}]\")\n",
        "\n",
        "    train_loss = train_fn(train_loader, model, optimizer, loss_fn)\n",
        "    val_loss, val_accuracy = val_fn(val_loader, model, loss_fn)\n",
        "\n",
        "    \"\"\"\n",
        "    (Opcional) Atualizar o scheduler\n",
        "    scheduler.step(val_loss)\n",
        "    \"\"\"\n",
        "\n",
        "    # Salvar o melhor modelo (baseado na menor perda de validação)\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_deeplab_model.pth\")\n",
        "        print(f\"Modelo salvo! Nova melhor perda de validação: {best_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Treinamento Concluído! ---\")\n",
        "print(f\"O melhor modelo foi salvo em 'best_deeplab_model.pth'\")"
      ],
      "metadata": {
        "id": "EW3tDghZZIGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação Visual e Inferência do Modelo\n",
        "\n",
        "Ter um modelo treinado e salvo é ótimo, mas como ele realmente se parece?\n",
        "\n",
        "Esta célula final de teste visual nos permite avaliar o desempenho qualitativo do modelo.\n",
        "\n",
        "1.  **Recriar a Arquitetura:** O script primeiro reconstrói a arquitetura do `deeplabv3_resnet50` e modifica a camada final para 3 classes, exatamente como fizemos antes do treinamento.\n",
        "2.  **Carregar Pesos:** Ele carrega os pesos do melhor modelo salvo (`best_deeplab_model.pth`).\n",
        "3.  **Modo de Avaliação:** O modelo é colocado em modo de inferência (`model.eval()`).\n",
        "4.  **Selecionar Amostras:** Ele pega algumas imagens aleatórias do `val_dataset` (o conjunto de validação que o modelo não viu durante o treino).\n",
        "5.  **Fazer a Predição:** Para cada imagem, ele:\n",
        "    * Envia a imagem para o modelo.\n",
        "    * Pega a saída `['out']` (logits).\n",
        "    * Usa `argmax(dim=1)` para encontrar a classe (0, 1, ou 2) com a maior pontuação para cada pixel.\n",
        "6.  **Plotar Comparação:** Por fim, ele exibe uma comparação lado a lado da **Imagem Original**, da **Máscara Real (Ground Truth)** e da **Máscara Prevista** pelo modelo."
      ],
      "metadata": {
        "id": "oidrmolP4JdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "import random\n",
        "\n",
        "# --- 1. Recarregar Configurações Essenciais ---\n",
        "# Estas variáveis devem corresponder às células anteriores do seu notebook\n",
        "\n",
        "# (Da Célula 13)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_CLASSES = 3  # (0: Fundo, 1: Crop, 2: Weed)\n",
        "\n",
        "# (Da Célula 17)\n",
        "MODEL_PATH = \"best_deeplab_model.pth\"\n",
        "\n",
        "# (Da Célula 10) - Usado para reverter a normalização para visualização\n",
        "MEAN = np.array([0.485, 0.456, 0.406])\n",
        "STD = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "# --- 2. Recarregar a arquitetura do modelo ---\n",
        "# Deve-se recriar a arquitetura EXATAMENTE como no treinamento\n",
        "print(\"Recriando a arquitetura do modelo...\")\n",
        "model = deeplabv3_resnet50(weights=None) # Os pesos serão inseridos, por isso None\n",
        "\n",
        "# Modificar a cabeça para corresponder ao NUM_CLASSES (da célula 13)\n",
        "in_channels = model.classifier[4].in_channels\n",
        "model.classifier[4] = nn.Conv2d(\n",
        "    in_channels,\n",
        "    NUM_CLASSES,\n",
        "    kernel_size=1,\n",
        "    stride=1\n",
        ")\n",
        "\n",
        "# --- 3. Carregar os pesos treinados ---\n",
        "try:\n",
        "    print(f\"Carregando pesos treinados de {MODEL_PATH}...\")\n",
        "    # Garante que o modelo seja carregado no dispositivo correto (CPU ou GPU)\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERRO: Arquivo do modelo '{MODEL_PATH}' não encontrado.\")\n",
        "    print(\"Certifique-se de que o treinamento foi concluído e o arquivo foi salvo.\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro inesperado ao carregar o modelo: {e}\")\n",
        "\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "print(\"Modelo pronto para inferência.\")\n",
        "\n",
        "# --- 4. Função para desnormalizar imagem (para visualização) ---\n",
        "def denormalize(tensor_image):\n",
        "    \"\"\"Reverte a normalização (A.Normalize) para visualização com Matplotlib.\"\"\"\n",
        "    # (C, H, W) -> (H, W, C)\n",
        "    img = tensor_image.cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "    img = (img * STD) + MEAN\n",
        "    # Garante que os valores fiquem entre [0, 1]\n",
        "    img = np.clip(img, 0, 1)\n",
        "    return img\n",
        "\n",
        "# --- 5. Teste Visual no Conjunto de Validação ---\n",
        "\n",
        "# Verifica se o 'val_dataset' (criado na célula 11) existe\n",
        "if 'val_dataset' not in globals():\n",
        "     print(\"ERRO: 'val_dataset' não está definido.\")\n",
        "     print(\"Por favor, execute a célula 11 ('Criação do Dataset e DataLoaders') primeiro.\")\n",
        "else:\n",
        "    num_samples_to_show = 3 # Quantas imagens de teste você quer ver\n",
        "    # Pega X índices aleatórios do dataset de validação\n",
        "    indices = random.sample(range(len(val_dataset)), num_samples_to_show)\n",
        "\n",
        "    print(f\"\\nMostrando {num_samples_to_show} predições aleatórias do conjunto de validação...\")\n",
        "\n",
        "    for idx in indices:\n",
        "        # O 'val_dataset' já aplica as transformações (redimensionar, normalizar)\n",
        "        image_tensor, gt_mask_tensor = val_dataset[idx]\n",
        "\n",
        "        # Preparar o tensor da imagem para o modelo\n",
        "        # Adiciona uma dimensão de \"lote\" (batch) [N, C, H, W]\n",
        "        input_tensor = image_tensor.unsqueeze(0).to(DEVICE) # (1, 3, 256, 256)\n",
        "\n",
        "        # --- 6. Executar Inferência ---\n",
        "        with torch.no_grad(): # Desliga o cálculo de gradientes\n",
        "            # A saída do deeplab é um dicionário\n",
        "            output = model(input_tensor)['out']\n",
        "\n",
        "        # --- 7. Pós-processar a saída ---\n",
        "        # output é (1, 3, H, W)\n",
        "        # .argmax(dim=1) pega o índice da classe com maior valor (0, 1 ou 2)\n",
        "        # para cada pixel, resultando em (1, H, W)\n",
        "        # .squeeze(0) remove a dimensão do lote -> (H, W)\n",
        "        pred_mask = output.argmax(dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Pegar a máscara real (ground truth)\n",
        "        gt_mask = gt_mask_tensor.cpu().numpy() # (H, W)\n",
        "\n",
        "        # Desnormalizar a imagem original para visualização\n",
        "        original_image_vis = denormalize(image_tensor)\n",
        "\n",
        "        # --- 8. Plotar os resultados ---\n",
        "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
        "        fig.suptitle(f\"Amostra de Validação (Índice: {idx})\", fontsize=16)\n",
        "\n",
        "        ax1.imshow(original_image_vis)\n",
        "        ax1.set_title(\"Imagem de Entrada (Processada)\")\n",
        "        ax1.axis('off')\n",
        "\n",
        "        # Usar vmin/vmax para garantir que as cores (0, 1, 2) sejam consistentes\n",
        "        ax2.imshow(gt_mask, vmin=0, vmax=NUM_CLASSES-1)\n",
        "        ax2.set_title(\"Máscara Real (Ground Truth)\")\n",
        "        ax2.axis('off')\n",
        "\n",
        "        ax3.imshow(pred_mask, vmin=0, vmax=NUM_CLASSES-1)\n",
        "        ax3.set_title(\"Máscara Prevista (Modelo)\")\n",
        "        ax3.axis('off')\n",
        "\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "yYz14w_94JBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}